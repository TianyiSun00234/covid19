{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt \nfrom scipy.optimize import minimize\n#import pythran\n\nimport sys\nnp.set_printoptions(threshold=sys.maxsize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"url_region_target = {\n    'global-confirmed' : 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv',\n    'global-fatalities' : 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv',\n    'global-recovered' : 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv',\n    'US-confirmed' : 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv',\n    'US-fatalities' : 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def data_processor_jhu(csv_url, Target):\n    df = pd.read_csv(csv_url)\n    df['Province/State'].fillna(\"\",inplace = True)\n    df['Country_Region'] = df['Country/Region'] + ' ' + df['Province/State']\n    df['Country_Region'] = df['Country_Region'].apply(lambda x : x.rstrip())\n    df = pd.concat([df['Country_Region'], df.iloc[:, 4:-1]], axis=1)\n    df = df.melt(id_vars='Country_Region').rename(columns={'variable':'Date', 'value': Target})\n    df['Date'] = pd.to_datetime(df['Date'])\n    return df\n\ndef data_processor_jhu_US(csv_url, Target):\n    df = pd.read_csv(csv_url)\n    df['Admin2'].fillna(\"\",inplace = True)\n    df['Country_Region'] = df['Country_Region'] + ' ' + df['Province_State'] + ' ' + df['Admin2']\n    df['Country_Region'] = df['Country_Region'].apply(lambda x : x.rstrip())\n    if Target == 'ConfirmedCases':\n        df = pd.concat([df['Country_Region'], df.iloc[:, 11:]], axis=1)\n    else:\n        df = pd.concat([df['Country_Region'], df.iloc[:, 12:]], axis=1)\n    df = df.melt(id_vars='Country_Region').rename(columns={'variable':'Date', 'value': Target})\n    df['Date'] = pd.to_datetime(df['Date'])\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_global_confirmed = data_processor_jhu(url_region_target['global-confirmed'], 'ConfirmedCases')\ndf_global_confirmed = df_global_confirmed.sort_values(by=['Country_Region', 'Date'])\ndf_global_fatalities = data_processor_jhu(url_region_target['global-fatalities'], 'Fatalities')\n#df_global_recovered = data_processor_jhu(url_region_target['global-recovered'], 'Recovered')\ndf_global = df_global_confirmed.merge(df_global_fatalities, left_on=['Country_Region', 'Date'], right_on=['Country_Region', 'Date'])\n#df_global = df_global_confirmed.merge(df_global_recovered, left_on=['Country_Region', 'Date'], right_on=['Country_Region', 'Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_us_confirmed = data_processor_jhu_US(url_region_target['US-confirmed'], 'ConfirmedCases')\ndf_us_confirmed = df_us_confirmed.sort_values(by=['Country_Region', 'Date'])\ndf_us_fatalities = data_processor_jhu_US(url_region_target['US-fatalities'], 'Fatalities')\ndf_us = df_us_confirmed.merge(df_us_fatalities, left_on=['Country_Region', 'Date'], right_on=['Country_Region', 'Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# No percentage change\ndf_raw = pd.concat([df_global, df_us], ignore_index=True).sort_values(by=['Country_Region', 'Date'])\n# df_raw['confirmed_diFF'] = df_raw.ConfirmedCases.groupby(df_raw.Country_Region).diff() \n# df_raw['fatalities_diFF'] = df_raw.Fatalities.groupby(df_raw.Country_Region).diff() \n# df_raw = df_raw.dropna()\ndf_raw","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"Synching\" with Kaggle's data for that juicy population column:"},{"metadata":{"trusted":false},"cell_type":"code","source":"url_kaggle = \"https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/20475/1267893/compressed/train.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1592319944&Signature=HqEVwXWYMMOl35YrhvdluYBSUsZ92qBmbZXw6fSeqmOQ8p3ElJLOgW31kLwdgrSueNx4LMZhqD8dGXmwUfesKSbikdMxV1lQ7z4bwRBy1IO4DHhOhvthsjb8Mog%2BBEkQHnML%2BQDC81bfXVubZsM6S4pfwHgfwktyIkfBDswJUe38UgddsUljhrLVorMXe%2BUEpc6dbNnf5BJlnE7zQlRCrjHOw3jQKismQF%2F%2FbArHunLJsB9%2BKPs%2B10r30RU1OVY6j12tK04w41gHlCo1XWWbrArXgpruklygtOXDjpzr%2Fu8Pe%2BRi6TYwfEX0fUz6Vcwbo2WwSrdf0k3VTV0RQFnZ3w%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain.csv.zip\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_kaggle = pd.read_csv(url_kaggle)\ndf_kaggle['Date'] = pd.to_datetime(df_kaggle['Date'])\ndf_kaggle['County'].fillna(\"\",inplace = True)\ndf_kaggle['Province_State'].fillna(\"\",inplace = True)\ndf_kaggle['Country_Region'] = df_kaggle['Country_Region'] + ' ' + df_kaggle['Province_State'] + ' ' + df_kaggle['County']\ndf_kaggle['Country_Region'] = df_kaggle['Country_Region'].apply(lambda x : x.rstrip())\ndf_kaggle.drop(columns=['County', 'Province_State'], inplace = True)\n\ndf_kaggle","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#temp_df = pd.DataFrame({'ConfirmedCases':df_kaggle['TargetValue'].iloc[::2].values, 'Fatalities':df_kaggle['TargetValue'].iloc[1::2].values})\ntemp_df = df_kaggle[df_kaggle.Target == 'ConfirmedCases'].drop(columns='Target').rename(columns={\"TargetValue\" : \"confirmed_diff\"})\ntemp_df_2 = df_kaggle[df_kaggle.Target == 'Fatalities'].drop(columns=['Id', 'Target', 'Weight', 'Population']).rename(columns={\"TargetValue\" : \"fatalities_diff\"})\ndf_kaggle_final = temp_df.merge(temp_df_2, left_on=['Country_Region', 'Date'], right_on=['Country_Region', 'Date'])\ndf_kaggle_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Combine everything\ndf_all = df_kaggle_final.merge(df_raw, left_on=['Country_Region', 'Date'], right_on=['Country_Region', 'Date'])\ndf_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Delete unused dfs\ndel temp_df, temp_df_2, df_kaggle_final, df_kaggle, df_raw, df_global_confirmed, df_global_fatalities, df_global, df_us_confirmed, df_us_fatalities, df_us","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting up the model - SIRD (Susceptible-Infected-Recovered-Dead)"},{"metadata":{},"cell_type":"markdown","source":"Details [here](https://www.overleaf.com/read/tnyfyyvggnsv)."},{"metadata":{},"cell_type":"markdown","source":"Using US New York for testing run:"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_nyc = df_all[df_all.Country_Region == 'US New York New York']\ndf_nyc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the sake of simplicity, assume $S(0) = N - I(0)$. Also take, $I(0)$ to be the first non-zero confirmed cases number."},{"metadata":{"trusted":false},"cell_type":"code","source":"df_nyc_train = df_nyc[df_nyc.ConfirmedCases > 0]\ndf_nyc_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting some initial condition from the dataframe:"},{"metadata":{"trusted":false},"cell_type":"code","source":"N = df_nyc_train.Population.iloc[0]\n#N = 850\n#N = 1\nI_0 = df_nyc_train.ConfirmedCases.iloc[0]\n#I_0 = df_nyc_train.ConfirmedCases.iloc[0]/df_nyc_train.Population.iloc[0]\n#I_0 = 0.001\nS_0 = N - I_0\nR_0 = 0\nD_0 = N - I_0 - S_0 - R_0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want $\\delta t$ to be small (say a quarter of a day):"},{"metadata":{"trusted":false},"cell_type":"code","source":"T = 1; \nr = 2**10; \nn = (df_nyc_train.shape[0]-1)*r;\ndt = T/n; \nDt = r*dt; \nM = int(n/r);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need tree sources of Gaussian noise, and let the number of sims be 100, so we are going to create 3 standard normal 100 by M matrices:"},{"metadata":{"trusted":false},"cell_type":"code","source":"def SIRD_Winc():\n    T = 1; \n    r = 2**10; \n    n = (df_nyc_train.shape[0]-1)*r;\n    dt = T/n; \n    Dt = r*dt; \n    M = int(n/r);\n    eta_1 = np.random.normal(0, 1, (5000, n)) * np.sqrt(dt)\n    eta_1 = np.add.reduceat(eta_1, np.arange(0, n, r), axis = 1)\n    eta_2 = np.random.normal(0, 1, (5000, n)) * np.sqrt(dt)\n    eta_2 = np.add.reduceat(eta_2, np.arange(0, n, r), axis = 1)\n    eta_3 = np.random.normal(0, 1, (5000, n)) * np.sqrt(dt)\n    eta_3 = np.add.reduceat(eta_3, np.arange(0, n, r), axis = 1)\n    Eta = [eta_1, eta_2, eta_3]\n    return Eta\n\nEtas = SIRD_Winc()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test with some dummy parameters:"},{"metadata":{"trusted":false},"cell_type":"code","source":"#7.52107377e+01, 2.28947946e+01, 6.96504586e-02, 8.33582662e+06\nbeta = 7.52107377e+01\ngamma = 2.28947946e+01\nmu =  6.96504586e-02","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"S = np.zeros((1000, M+1))\nI = np.zeros((1000, M+1))\nR = np.zeros((1000, M+1))\nD = np.zeros((1000, M+1))\n\n#S[:,0] = S_0\nS[:,0] = 8.33582662e+06\nI[:,0] = I_0\nR[:,0] = R_0\nD[:,0] = D_0\n\nfor i in range(1, M+1):\n    S[:, i] = S[:, i-1] - (beta * I[:, i-1] * S[:, i-1]/N) * Dt + np.sqrt(beta * I[:, i-1] * S[:, i-1]/N) * eta_1[:,i-1]\n    #S[S[:, i] > N] = N\n    I[:, i] = I[:, i-1] + (beta * I[:, i-1] * S[:, i-1]/N - (gamma + mu) * I[:, i-1]) * Dt - np.sqrt(beta * I[:, i-1] * S[:, i-1]/N) * eta_1[:,i-1] + \\\n              np.sqrt((gamma + mu)*I[:, i-1]) * eta_2[:,i-1]\n    I[I[:, i] < 0] = 0\n    R[:, i] = R[:, i-1] + gamma * I[:, i-1] * Dt - gamma * np.sqrt((gamma + mu) * I[:, i-1]) /(gamma + mu) * eta_2[:,i-1] + \\\n              np.sqrt(gamma*mu*I[:, i-1]/(gamma + mu)) * eta_3[:,i-1]\n    R[R[:, i] < 0] = 0\n#     S[:, i] = S[:, i-1] - (beta * I[:, i-1] * S[:, i-1]) * dt + np.sqrt(beta * I[:, i-1] * S[:, i-1]) * np.sqrt(dt) * eta_1[:,i-1]\n#     I[:, i] = I[:, i-1] + (beta * I[:, i-1] * S[:, i-1] - (gamma + mu) * I[:, i-1]) * dt - np.sqrt(beta * I[:, i-1] * S[:, i-1]) * np.sqrt(dt) * eta_1[:,i-1] + \\\n#               np.sqrt((gamma + mu)*I[:, i-1]) * np.sqrt(dt) * eta_2[:,i-1]\n#     R[:, i] = R[:, i-1] + gamma * I[:, i-1] * dt - gamma * np.sqrt((gamma + mu) * I[:, i-1]) /(gamma + mu) * np.sqrt(dt) * eta_2[:,i-1] + \\\n#               np.sqrt(gamma*mu*I[:, i-1]/(gamma + mu)) * np.sqrt(dt) * eta_3[:,i-1]\n    D[:, i] = N - S[:, i] - I[:, i] - R[:, i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x = range(101)\nplt.plot(x, np.mean(D[:, 0:],axis=0).T, label='Modeled Fitted Fatalities');\nplt.plot(x, df_nyc_train.Fatalities, label='US NYC Fatalities');\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(x, np.mean(R[:, 0:],axis=0).T, label = 'R');\nplt.plot(x, np.mean(I[:, 0:],axis=0).T, label = 'I');\nplt.plot(x, np.mean(S[:, 0:],axis=0).T, label = 'S');\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(x, D[:, 0:].T, label = 'S');","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def SIRD(sims_no, days, pop, init_inf, params, winc):\n    # sims_no: number of simulations, recommended 10000 at least\n    # dt: size of time steps - in fraction of a day\n    # params: beta, gamma, mu, S_0\n    \n    beta = params[0]\n    gamma = params[1]\n    mu = params[2]\n    N = pop\n    \n    T = 1; \n    r = 2**10; \n    #n = (df_nyc_train.shape[0]-1)*r;\n    n = days*r\n    dt = T/n; \n    Dt = r*dt; \n    M = int(n/r);\n    \n#     eta_1 = np.random.normal(0, 1, (sims_no, n)) * np.sqrt(dt)\n#     eta_1 = np.add.reduceat(eta_1, np.arange(0, n, r), axis = 1)\n#     eta_2 = np.random.normal(0, 1, (sims_no, n)) * np.sqrt(dt)\n#     eta_2 = np.add.reduceat(eta_2, np.arange(0, n, r), axis = 1)\n#     eta_3 = np.random.normal(0, 1, (sims_no, n)) * np.sqrt(dt)\n#     eta_3 = np.add.reduceat(eta_3, np.arange(0, n, r), axis = 1)\n    \n    eta_1 = winc[0]\n    eta_2 = winc[1]\n    eta_3 = winc[2]\n    \n    S = np.zeros((sims_no, M+1))\n    I = np.zeros((sims_no, M+1))\n    R = np.zeros((sims_no, M+1))\n    D = np.zeros((sims_no, M+1))\n    \n    S_0 = params[3]\n    I_0 = init_inf\n    R_0 = 0\n    D_0 = 0\n    \n    S[:,0] = S_0\n    I[:,0] = I_0\n    R[:,0] = R_0\n    D[:,0] = D_0\n    \n    for i in range(1, M+1):\n        S[:, i] = S[:, i-1] - (beta * I[:, i-1] * S[:, i-1]/N) * Dt + np.sqrt(beta * I[:, i-1] * S[:, i-1]/N) * eta_1[:,i-1]\n        S[S[:, i] < 0] = 0\n        I[:, i] = I[:, i-1] + (beta * I[:, i-1] * S[:, i-1]/N - (gamma + mu) * I[:, i-1]) * Dt - np.sqrt(beta * I[:, i-1] * S[:, i-1]/N) * eta_1[:,i-1] + \\\n                  np.sqrt((gamma + mu)*I[:, i-1]) * eta_2[:,i-1]\n        I[I[:, i] < 0] = 0\n        R[:, i] = R[:, i-1] + gamma * I[:, i-1] * Dt - gamma * np.sqrt((gamma + mu) * I[:, i-1]) /(gamma + mu) * eta_2[:,i-1] + \\\n                  np.sqrt(gamma*mu*I[:, i-1]/(gamma + mu)) * eta_3[:,i-1]\n        R[R[:, i] < 0] = 0\n        D[:, i] = N - S[:, i] - I[:, i] - R[:, i]\n        D[D[:, i] < 0] = 0\n    return S, I, R, D\n\ndef params_optim(df):\n    pop = df.Population.iloc[0]\n    deaths_time_series = df.Fatalities\n    initial_infected = df.ConfirmedCases.iloc[0]\n    \n    T = 1;\n    r = 2**10; \n    n = (df.shape[0]-1)*r\n    dt = T/n; \n    Dt = r*dt; \n    M = int(n/r);\n    sims = 5000\n\n    winc_1 = np.random.normal(0, 1, (sims, n)) * np.sqrt(dt)\n    winc_1 = np.add.reduceat(winc_1, np.arange(0, n, r), axis = 1)\n    winc_2 = np.random.normal(0, 1, (sims, n)) * np.sqrt(dt)\n    winc_2 = np.add.reduceat(winc_2, np.arange(0, n, r), axis = 1)\n    winc_3 = np.random.normal(0, 1, (sims, n)) * np.sqrt(dt)\n    winc_3 = np.add.reduceat(winc_3, np.arange(0, n, r), axis = 1)\n    \n    Winc = [winc_1, winc_2, winc_3]\n\n    def target_func(params):\n        _, _, _, D= SIRD(sims, df.shape[0]-1, pop, initial_infected, params, Winc)\n        D_mean = np.mean(D,axis=0) \n        return np.sum((deaths_time_series - D_mean)**2)\n    \n    x0 = np.array([40, 20, 1, pop])\n    bnds = ((0.0000001, None), (0.0000001, None), (0.0000001, None), (0.0000001, pop))\n    res = minimize(target_func, x0, bounds=bnds, options={'ftol': 1e-10, 'disp': True, 'maxiter' : 20000})\n    \n    return res\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"results = params_optim(df_nyc_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"S,I,R,D = SIRD(5000, df_nyc_train.shape[0]-1, df_nyc_train.Population.iloc[0], df_nyc_train.ConfirmedCases.iloc[0], results.x, Etas)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}